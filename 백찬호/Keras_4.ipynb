{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1c240b8-4cf3-424d-876c-aed304ccff34",
   "metadata": {},
   "source": [
    "데이터가 적을 때 사용하는 평가방법 3가지\n",
    "* 단순 홀드아웃 검증\n",
    "* K겹 교차검증\n",
    "* 셔플링을 사용한 반복 K겹 교차검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09485af-a9e1-401c-a815-0f827de434c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#홀드아웃 검증 구현 예\n",
    "import numpy as np\n",
    "\n",
    "num_validation_samples = 10000\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "validation_data = data[:num_validation_samples] #검증 세트 만들기\n",
    "data = data[num_validation_samples:]\n",
    "\n",
    "training_data = data[:] #훈련세트 만들기\n",
    "\n",
    "model=get_model()\n",
    "model.train(training_data)\n",
    "validation_score = model.evaluate(validation_data)\n",
    "\n",
    "# 여기에서 모델을 튜닝하고,\n",
    "# 다시 훈련하고, 평가하고, 또 다시 튜닝하고... \n",
    "\n",
    "model = get_model()\n",
    "model.train(np.concatenate([training_data, \n",
    "                            validation_data])) #하이퍼파라미터 튜닝이 끝나면 테스트데이터를 제외한 모든 데이터를 사용해 모델을 다시 다시 훈련\n",
    "test_score = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3005e32b-9f9e-460d-96fc-448b9d8751f6",
   "metadata": {},
   "source": [
    "홀드아웃 검증방법에서는 데이터의 일정량을 테스트 세트로 떼어놓고 남은 데이터로 훈련하고 테스트 세트로 평가한다.\n",
    "* 데이터가 적을 때는 검증세트와 테스트세트 샘플이 너무 적어 주어진 전체 데이터를 통계적으로 대표하지 못할 수 있다. \n",
    "\n",
    "=>다른 난수 초깃값으로 셔플링해서 데이터를 나눌때 모델 성능이 매우 달라지면 이 문제를 가진 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270cdf25-e441-4ace-ad04-a8a6411e9cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K겹 교차검증 구현 예\n",
    "k = 4\n",
    "num_validation_samples = len(data) // k\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "validation_scores = []\n",
    "for fold in range(k):\n",
    "    \n",
    "    validation_data = data[num_validation_samples * fold: num_validation_samples * (fold + 1)]\n",
    "    training_data = data[:num_validation_samples * fold] + \n",
    "    data[num_validation_samples * (fold + 1):]\n",
    "\n",
    "    model = get_model()\n",
    "    model.train(training_data)\n",
    "    validation_score = model.evaluate(validation_scores)\n",
    "    validaton_scores.append(validation_score)\n",
    "\n",
    "    validation_score = np.average(validation_scores)\n",
    "\n",
    "    model = get_model()\n",
    "    model.train(data)\n",
    "    test_score = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5d2a70-65a7-45c2-9871-db8bd4de2d71",
   "metadata": {},
   "source": [
    "셔플링을 사용한 반복 K겹 교차검증: 가용데이터가 적고 가능한 정확히 모델을 평가하려 할 때 사용. K겹 교차검증을 여러번 적용하되 K개의 분할로 나누기 전에 매번 데이터를 무작위로 섞는다. 결국 PxK개의 모델(P는 반복횟수)을 훈련하고 평가한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba973d9-16e5-47c5-b714-de8a3c4ffe58",
   "metadata": {},
   "source": [
    "영화리뷰 분류 모델 네트워크 크기 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1882f713-c737-43a1-abdf-267e4771b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 모델\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9ca9d2a-7730-4cf1-a606-6912d2dc000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 작은 용량의 모델\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(6, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(6, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae202a9f-be33-4fa6-b2ac-8134f0eda72b",
   "metadata": {},
   "source": [
    "작은 네트워크가 더 나중에 과대적합되기 시작한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbd4b73f-76d7-4a17-bce8-d82d69d746b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 큰 용량의 모델\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(1024, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6335595-9b12-4fe4-aee7-69a48796ad50",
   "metadata": {},
   "source": [
    "첫 번째 에포크 이후 거의 바로 과대적합이 시작되고 검증손실도 매우 불안정. 훈련 손실은 매우 빠르게 0에 가까워지지만 더욱 과대적합에 민감해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ea0cf8-88cb-47de-bd1a-6ba9954fb512",
   "metadata": {},
   "source": [
    "간단한 모델이 복잡한 모델보다 덜 과대적합될 가능성이 높고 간단한 모델은 적은 수의 파라미터를 가진 모델, 파라미터 값 분포 엔트로피가 작은 모델을 말함. 네트워크 복잡도에 제한을 둬서 가중치가 작은값을 가지도록 강제하는 것을 가중치 규제라고 하고 이에는 두가지 형태가 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b38a52-c8de-425e-8180-181bc7856e7c",
   "metadata": {},
   "source": [
    "* L1규제: 가중치의 절댓값에 비례하는 비용이 추가됨\n",
    "* L2규제: 가중치의 제곱에 비레하는 비용이 추가되고 가중치 감쇠라고도 부름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db4f6a1c-0e7c-4adc-aa47-95dc5aed7c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 L2 가중치 추가하기\n",
    "from keras import regularizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                       activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                       activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236bcc63-8366-4587-8fa2-b3fca941ff03",
   "metadata": {},
   "source": [
    "과대적합에 훨씬 잘 견딘다 /  l2(0.001)은 가중치 행렬의 모든 원소를 제곱하고 0.001을 곱해서 네트워크 전체 손실에 더해진다는 의미이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23083796-3da0-4fe8-8387-cdcebbedd4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.regularizers.L1L2 at 0x2638b1e4f28>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 케라스에서 사용할 수 있는 가중치 규제\n",
    "from keras import regularizers\n",
    "\n",
    "regularizers.l1(0.001) # L1 규제\n",
    "regularizers.l1_l2(l1=0.001, l2=0.001) # L1 L2 규제 병행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f3b4b7-8419-4db8-b41c-3e11d080a173",
   "metadata": {},
   "source": [
    "드롭아웃:훈련하는 동안 무작위로 층의 일부 훈련 특성을 제외시킨다(0으로 만듬). 드롭아웃 비율은 0.2~0.5로 테스트단계에서는 드롭아웃되지 않지만 훈련할 때보다 더 많은 유닛이 활성화 되기 때문에 층의 출력을 드롭아웃 비율에 비례해 줄여줌\n",
    "* 층의 추력 값에 노이즈를 추가해 중요하지 ㅇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c779828-034a-45f8-8fcb-2fa13a6847a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB 네트워크에 드롭아웃 추가하기\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
